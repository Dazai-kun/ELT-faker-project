{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from minio import Minio\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "import os\n",
    "import io\n",
    "from datetime import datetime, timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = duckdb.connect('~/my-first-elt-project/docker/db/duckdb/duckdb_dw.duckdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Minio(endpoint=\"localhost:9000\", access_key='a5926TSNVC2r9J4Y2Eqh', secret_key='3YBQqcerjz5TsV8X851gi3Rl7YNclYQ6UD1MrEPY', secure=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\n",
    "    'postgresql+psycopg2://oltp:oltp@localhost:5432/oltp'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINITIONS\n",
    "tbl_list = ['users', 'products', 'transactions', 'transaction_detail']\n",
    "bucket_name = 'snapshot'\n",
    "\n",
    "#DEFINE CURRENT TIME VARIABLES\n",
    "current_dt_string = datetime.now()\n",
    "current_date_full = current_dt_string.strftime(\"%d%m%Y%H%M%S\")\n",
    "current_year = current_dt_string.strftime(\"%Y\")\n",
    "current_month = current_dt_string.strftime(\"%m\")\n",
    "current_date = current_dt_string.strftime(\"%d\")\n",
    "\n",
    "#DEFINE PREVIOUS DAY TIME VARIABLES\n",
    "\n",
    "prev_dt_string = current_dt_string - timedelta(days=1)\n",
    "prev_full_date = prev_dt_string.strftime(\"%d%m%Y%H%M%S\")\n",
    "prev_year = prev_dt_string.strftime(\"%Y\") # prev_year means that the year that last day's data was on, not literally 'last year'\n",
    "prev_month = prev_dt_string.strftime(\"%m\") # the same rule applies for prev_month and prev_date.\n",
    "prev_date = prev_dt_string.strftime(\"%d\")\n",
    "\n",
    "#DEFINE T-2 DAY TIME VARIABLES\n",
    "t2_dt_string = prev_dt_string - timedelta(days=1)\n",
    "t2_full_date = prev_dt_string.strftime(\"%d%m%Y%H%M%S\")\n",
    "t2_year = prev_dt_string.strftime(\"%Y\") # prev_year means that the year that last day's data was on, not literally 'last year'\n",
    "t2_month = prev_dt_string.strftime(\"%m\") # the same rule applies for prev_month and prev_date.\n",
    "t2_date = prev_dt_string.strftime(\"%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_df_for_t1_day(table_name):\n",
    "    df_t1 = pd.DataFrame()\n",
    "    table = table_name\n",
    "    # for table in tbl_list:\n",
    "        # t1_dt_obj_list = client.list_objects(bucket_name=bucket_name, prefix=f'{bucket_name}/{table}/2024/06/08/')\n",
    "    t1_dt_obj_list = client.list_objects(bucket_name=bucket_name, prefix=f'{bucket_name}/{table}/{prev_year}/{prev_month}/{prev_date}/')\n",
    "    for obj in t1_dt_obj_list:            \n",
    "        file_path = os.path.basename(obj.object_name)\n",
    "        object_key = obj.object_name\n",
    "        print(f'object_key: {object_key}, file_path: {file_path}')\n",
    "\n",
    "        try:\n",
    "            response_t1 = client.get_object(bucket_name,object_name=object_key)\n",
    "            buffer = io.BytesIO(response_t1.read())\n",
    "            df = pd.read_parquet(buffer)\n",
    "            df_t1 = pd.concat([df_t1, df], ignore_index=True) #.reset_index(drop=True)\n",
    "            print(df_t1)       \n",
    "        \n",
    "        finally:\n",
    "            response_t1.close()\n",
    "            buffer.close()\n",
    "            response_t1.release_conn()\n",
    "    return df_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_for_t2_day(table_name):\n",
    "    df_t2 = pd.DataFrame()\n",
    "    table = table_name\n",
    "    # for table in tbl_list:\n",
    "    t2_dt_obj_list = client.list_objects(bucket_name=bucket_name, \n",
    "                                         prefix=f'{bucket_name}/{table}/{t2_year}/{t2_month}/{t2_date}/')\n",
    "    for obj in t2_dt_obj_list:            \n",
    "        file_path = os.path.basename(obj.object_name)\n",
    "        object_key = obj.object_name\n",
    "        print(f'object_key: {object_key}, file_path: {file_path}')\n",
    "\n",
    "        try:\n",
    "            response_t2 = client.get_object(bucket_name, object_name=object_key)\n",
    "            buffer = io.BytesIO(response_t2.read())\n",
    "            df = pd.read_parquet(buffer)\n",
    "            df_t2 = pd.concat([df_t2, df], ignore_index=True)\n",
    "            print(df_t2)       \n",
    "            \n",
    "        finally:\n",
    "            response_t2.close()\n",
    "            buffer.close()\n",
    "            response_t2.release_conn()\n",
    "    return df_t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_for_changes(table_name):\n",
    "    tbl = table_name\n",
    "    df1 = create_df_for_t1_day(tbl)\n",
    "    df2= create_df_for_t2_day(tbl)\n",
    "    changes = df1[~df1.apply(tuple, 1).isin(df2.apply(tuple, 1))]\n",
    "    return changes\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = duckdb.connect('~/my-first-elt-project/docker/db/duckdb/duckdb_dw.duckdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_sql(df, table_name):\n",
    "    a = []\n",
    "    target_table_name = table_name\n",
    "    temp_table = f\"{table_name}_temporary_table\"\n",
    "    for col in df.columns:\n",
    "        a.append(f'\"{col}\"=EXCLUDED.\"{col}\"')\n",
    "    conn.execute(f\"CREATE TABLE {temp_table} AS SELECT * FROM df\")\n",
    "    conn.execute(f\"INSERT INTO {temp_table} SELECT * FROM df\")\n",
    "    \n",
    "    #df.to_sql(temp_table, engine, if_exists='replace', index=False) \n",
    "    set_statement = ', '.join(a)\n",
    "    upsert_query = f\"\"\"\n",
    "    WITH primary_col_extract AS (\n",
    "        SELECT a.attname as key_name\n",
    "        FROM   pg_index i\n",
    "        JOIN   pg_attribute a ON a.attrelid = i.indrelid\n",
    "                            AND a.attnum = ANY(i.indkey)\n",
    "        WHERE  i.indrelid = {target_table_name}::regclass\n",
    "        AND    i.indisprimary\n",
    "    )\n",
    "    INSERT INTO {target_table_name} as tt \n",
    "    SELECT * FROM {temp_table} as tmp\n",
    "    ON CONFLICT\n",
    "    DO UPDATE SET \"\"\"\n",
    "    # combined_query = upsert_query\n",
    "    print(upsert_query + set_statement)\n",
    "    conn.execute(upsert_query + set_statement)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elt_process():\n",
    "    with open('/home/admin/my-first-elt-project/docker/db/oltp_schema.sql', 'r') as file:\n",
    "        sql_script = file.read()\n",
    "    conn.execute(sql_script)\n",
    "    for t in tbl_list:\n",
    "        change_df = create_df_for_changes(t)\n",
    "        upsert_sql(change_df,t)\n",
    "    conn.close()\n",
    "    print('data has been upserted successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    WITH primary_col_extract AS (\n",
      "        SELECT a.attname as key_name\n",
      "        FROM   pg_index i\n",
      "        JOIN   pg_attribute a ON a.attrelid = i.indrelid\n",
      "                            AND a.attnum = ANY(i.indkey)\n",
      "        WHERE  i.indrelid = ['users', 'products', 'transactions', 'transaction_detail']::regclass\n",
      "        AND    i.indisprimary\n",
      "    )\n",
      "    INSERT INTO ['users', 'products', 'transactions', 'transaction_detail'] as tt \n",
      "    SELECT * FROM ['users', 'products', 'transactions', 'transaction_detail'] as tmp\n",
      "    ON CONFLICT\n",
      "    DO UPDATE SET \"column1\" = EXCLUDED.\"column1\", \"column2\" = EXCLUDED.\"column2\", \"column3\" = EXCLUDED.\"column3\"\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'column1': [1, 2, 3],\n",
    "    'column2': ['A', 'B', 'C'],\n",
    "    'column3': [1.1, 2.2, 3.3]\n",
    "})\n",
    "\n",
    "a= ', '.join([f'\"{col}\" = EXCLUDED.\"{col}\"' for col in df.columns])\n",
    "b = f\"\"\"\n",
    "    WITH primary_col_extract AS (\n",
    "        SELECT a.attname as key_name\n",
    "        FROM   pg_index i\n",
    "        JOIN   pg_attribute a ON a.attrelid = i.indrelid\n",
    "                            AND a.attnum = ANY(i.indkey)\n",
    "        WHERE  i.indrelid = {tbl_list}::regclass\n",
    "        AND    i.indisprimary\n",
    "    )\n",
    "    INSERT INTO {tbl_list} as tt \n",
    "    SELECT * FROM {tbl_list} as tmp\n",
    "    ON CONFLICT\n",
    "    DO UPDATE SET \"\"\"\n",
    "print(b + a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object_key: snapshot/users/2024/06/17/users_snapshot_17062024051430.parquet, file_path: users_snapshot_17062024051430.parquet\n",
      "          id               name                         email  \\\n",
      "0     IZ6732       EUGENE HUANG  eugene10@adventure-works.com   \n",
      "1     GT9653     BLAKE ANDERSON         bakerjody@example.net   \n",
      "2     ZZ7643             AMY YE          amanda67@example.net   \n",
      "3     WS4471    WENDY DOMINGUEZ     roblesdouglas@example.com   \n",
      "4     CI8362  Jason Matthews MD            ljames@example.com   \n",
      "...      ...                ...                           ...   \n",
      "3191  SM4946        Lisa Keller    sheilaanderson@example.com   \n",
      "3192  FR1629     Jennifer Greer   stephensvalerie@example.net   \n",
      "3193  CD2614   Sarah Strickland           laura06@example.org   \n",
      "3194  ZA0548        Renee Allen     arnolddeborah@example.com   \n",
      "3195  NF1102     Theresa Dawson         mmartinez@example.org   \n",
      "\n",
      "              created_at  \n",
      "0    2024-05-15 13:34:00  \n",
      "1    2024-05-15 18:30:00  \n",
      "2    2024-05-15 18:18:00  \n",
      "3    2024-05-15 15:46:00  \n",
      "4    2024-06-17 08:01:02  \n",
      "...                  ...  \n",
      "3191 2024-06-18 05:01:01  \n",
      "3192 2024-06-18 05:01:01  \n",
      "3193 2024-06-18 05:01:01  \n",
      "3194 2024-06-18 05:01:01  \n",
      "3195 2024-06-18 05:01:01  \n",
      "\n",
      "[3196 rows x 4 columns]\n",
      "object_key: snapshot/users/2024/06/17/users_snapshot_17062024051430.parquet, file_path: users_snapshot_17062024051430.parquet\n",
      "          id               name                         email  \\\n",
      "0     IZ6732       EUGENE HUANG  eugene10@adventure-works.com   \n",
      "1     GT9653     BLAKE ANDERSON         bakerjody@example.net   \n",
      "2     ZZ7643             AMY YE          amanda67@example.net   \n",
      "3     WS4471    WENDY DOMINGUEZ     roblesdouglas@example.com   \n",
      "4     CI8362  Jason Matthews MD            ljames@example.com   \n",
      "...      ...                ...                           ...   \n",
      "3191  SM4946        Lisa Keller    sheilaanderson@example.com   \n",
      "3192  FR1629     Jennifer Greer   stephensvalerie@example.net   \n",
      "3193  CD2614   Sarah Strickland           laura06@example.org   \n",
      "3194  ZA0548        Renee Allen     arnolddeborah@example.com   \n",
      "3195  NF1102     Theresa Dawson         mmartinez@example.org   \n",
      "\n",
      "              created_at  \n",
      "0    2024-05-15 13:34:00  \n",
      "1    2024-05-15 18:30:00  \n",
      "2    2024-05-15 18:18:00  \n",
      "3    2024-05-15 15:46:00  \n",
      "4    2024-06-17 08:01:02  \n",
      "...                  ...  \n",
      "3191 2024-06-18 05:01:01  \n",
      "3192 2024-06-18 05:01:01  \n",
      "3193 2024-06-18 05:01:01  \n",
      "3194 2024-06-18 05:01:01  \n",
      "3195 2024-06-18 05:01:01  \n",
      "\n",
      "[3196 rows x 4 columns]\n",
      "\n",
      "    WITH primary_col_extract AS (\n",
      "        SELECT a.attname as key_name\n",
      "        FROM   pg_index i\n",
      "        JOIN   pg_attribute a ON a.attrelid = i.indrelid\n",
      "                            AND a.attnum = ANY(i.indkey)\n",
      "        WHERE  i.indrelid = users::regclass\n",
      "        AND    i.indisprimary\n",
      "    )\n",
      "    INSERT INTO users as tt \n",
      "    SELECT * FROM users_temporary_table as tmp\n",
      "    ON CONFLICT\n",
      "    DO UPDATE SET \"id\"=EXCLUDED.\"id\", \"name\"=EXCLUDED.\"name\", \"email\"=EXCLUDED.\"email\", \"created_at\"=EXCLUDED.\"created_at\"\n"
     ]
    },
    {
     "ename": "BinderException",
     "evalue": "Binder Error: Can not assign to column 'id' because it has a UNIQUE/PRIMARY KEY constraint",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBinderException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43melt_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[48], line 7\u001b[0m, in \u001b[0;36melt_process\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tbl_list:\n\u001b[1;32m      6\u001b[0m     change_df \u001b[38;5;241m=\u001b[39m create_df_for_changes(t)\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mupsert_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchange_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m conn\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata has been upserted successfully\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[95], line 27\u001b[0m, in \u001b[0;36mupsert_sql\u001b[0;34m(df, table_name)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# combined_query = upsert_query\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(upsert_query \u001b[38;5;241m+\u001b[39m set_statement)\n\u001b[0;32m---> 27\u001b[0m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupsert_query\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mset_statement\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mBinderException\u001b[0m: Binder Error: Can not assign to column 'id' because it has a UNIQUE/PRIMARY KEY constraint"
     ]
    }
   ],
   "source": [
    "elt_process()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
